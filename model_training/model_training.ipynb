{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T20:28:53.436937Z",
     "start_time": "2025-03-24T20:28:52.056780Z"
    }
   },
   "source": [
    "# Environment Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, classification_report, \n",
    "                             confusion_matrix, f1_score)\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import os\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "base_path = \"../dataset/data_exploration/\"\n",
    "metadata_path = \"../dataset/dataverse_files/\"\n",
    "\n",
    "# Load feature files\n",
    "color_var = pd.read_csv(os.path.join(base_path, \"color_variance_features.csv\"))\n",
    "color_hist = pd.read_csv(os.path.join(base_path, \"combined_color_histogram_features.csv\"))\n",
    "lbp = pd.read_csv(os.path.join(base_path, \"combined_lbp_features.csv\"))\n",
    "glcm = pd.read_csv(os.path.join(base_path, \"glcm_features.csv\"))\n",
    "metadata = pd.read_csv(os.path.join(metadata_path, \"HAM10000_metadata\"))\n",
    "\n",
    "# Function to extract image_id from file_name\n",
    "def extract_image_id(file_name):\n",
    "    # Extract the base name (e.g., 'ISIC_0024306.jpg')\n",
    "    base_name = file_name.split('\\\\')[-1]\n",
    "    # Remove the file extension (e.g., '.jpg')\n",
    "    image_id = os.path.splitext(base_name)[0]\n",
    "    return image_id\n",
    "\n",
    "# Apply the function to extract image_id\n",
    "color_var['image_id'] = color_var['file_name'].apply(extract_image_id)\n",
    "color_hist['image_id'] = color_hist['file_name'].apply(extract_image_id)\n",
    "lbp['image_id'] = lbp['file_name'].apply(extract_image_id)\n",
    "glcm['image_id'] = glcm['file_name'].apply(extract_image_id)\n",
    "\n",
    "# Sort feature DataFrames by image_id\n",
    "color_var_sorted = color_var.sort_values(by='image_id').reset_index(drop=True)\n",
    "color_hist_sorted = color_hist.sort_values(by='image_id').reset_index(drop=True)\n",
    "lbp_sorted = lbp.sort_values(by='image_id').reset_index(drop=True)\n",
    "glcm_sorted = glcm.sort_values(by='image_id').reset_index(drop=True)\n",
    "\n",
    "# Sort metadata by image_id\n",
    "metadata_sorted = metadata.sort_values(by='image_id').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature image_id after sorting: 0    ISIC_0024306\n",
      "1    ISIC_0024307\n",
      "2    ISIC_0024308\n",
      "3    ISIC_0024309\n",
      "4    ISIC_0024310\n",
      "Name: image_id, dtype: object\n",
      "Metadata image_id after sorting: 0    ISIC_0024306\n",
      "1    ISIC_0024307\n",
      "2    ISIC_0024308\n",
      "3    ISIC_0024309\n",
      "4    ISIC_0024310\n",
      "Name: image_id, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check for consistency\n",
    "print(\"Feature image_id after sorting:\", color_var_sorted['image_id'].head())\n",
    "print(\"Metadata image_id after sorting:\", metadata_sorted['image_id'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/tpb8r43x2wg8jsscbc0l8glc0000gn/T/ipykernel_68973/1960999740.py:7: FutureWarning: Passing 'suffixes' which cause duplicate columns {'file_name_x', 'folder_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = pd.merge(merged, df, on='image_id', how='inner')\n"
     ]
    }
   ],
   "source": [
    "# Merge all features\n",
    "def merge_features(df_list):\n",
    "    # Start with the first DataFrame\n",
    "    merged = df_list[0]\n",
    "    # Merge the rest\n",
    "    for df in df_list[1:]:\n",
    "        merged = pd.merge(merged, df, on='image_id', how='inner')\n",
    "    return merged\n",
    "\n",
    "# List of sorted feature DataFrames\n",
    "feature_dfs = [color_var_sorted, color_hist_sorted, lbp_sorted, glcm_sorted]\n",
    "merged_features = merge_features(feature_dfs)\n",
    "\n",
    "# Add metadata (dx column)\n",
    "full_data = merged_features.copy()\n",
    "full_data['dx'] = metadata_sorted['dx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Data Shape: (10015, 128)\n",
      "Full Data Columns: Index(['file_name_x', 'folder_x', 'mean_r', 'mean_g', 'mean_b', 'var_r',\n",
      "       'var_g', 'var_b', 'overall_var', 'image_id',\n",
      "       ...\n",
      "       'lbp_8', 'lbp_9', 'file_name_y', 'folder_y', 'contrast',\n",
      "       'dissimilarity', 'homogeneity', 'energy', 'correlation', 'dx'],\n",
      "      dtype='object', length=128)\n",
      "Class Distribution:\n",
      " nv       6705\n",
      "mel      1113\n",
      "bkl      1099\n",
      "bcc       514\n",
      "akiec     327\n",
      "vasc      142\n",
      "df        115\n",
      "Name: dx, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the full data\n",
    "print(\"Full Data Shape:\", full_data.shape)\n",
    "print(\"Full Data Columns:\", full_data.columns)\n",
    "print(\"Class Distribution:\\n\", full_data['dx'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed_cols: ['dx', 'file_name_y', 'image_id', 'folder_x', 'file_name_x', 'folder_y']\n",
      "X Shape: (10015, 119)\n",
      "y Shape: (10015,)\n",
      "X Columns: Index(['mean_r', 'mean_g', 'mean_b', 'var_r', 'var_g', 'var_b', 'overall_var',\n",
      "       'hist_r_0', 'hist_r_1', 'hist_r_2',\n",
      "       ...\n",
      "       'lbp_6', 'lbp_7', 'lbp_8', 'lbp_9', 'contrast', 'dissimilarity',\n",
      "       'homogeneity', 'energy', 'correlation', 'label'],\n",
      "      dtype='object', length=119)\n",
      "Class Distribution:\n",
      " 5    6705\n",
      "4    1113\n",
      "2    1099\n",
      "1     514\n",
      "0     327\n",
      "6     142\n",
      "3     115\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "full_data['label'] = le.fit_transform(full_data['dx'])\n",
    "\n",
    "# Define X and y (features and target)\n",
    "X = full_data\n",
    "y = full_data['label']\n",
    "\n",
    "# Drop non-feature columns\n",
    "X_numeric = X.select_dtypes(include=['number'])\n",
    "\n",
    "# Output removed columns\n",
    "removed_cols = list(set(X.columns) - set(X_numeric.columns))\n",
    "print(\"removed_cols:\", removed_cols)\n",
    "\n",
    "# Update X\n",
    "X = X_numeric\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"X Shape:\", X.shape)\n",
    "print(\"y Shape:\", y.shape)\n",
    "print(\"X Columns:\", X.columns)\n",
    "print(\"Class Distribution:\\n\", y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Pipeline Setup\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# SVM Pipeline\n",
    "svm_pipe = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('svm', SVC(probability=True, random_state=SEED))\n",
    "])\n",
    "\n",
    "# Random Forest Pipeline\n",
    "rf_pipe = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('rf', RandomForestClassifier(random_state=SEED))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "# SVM Hyperparameters\n",
    "svm_params = {\n",
    "    'svm__C': [0.01],\n",
    "    'svm__kernel': ['linear'],\n",
    "    'svm__gamma': ['scale'],\n",
    "    'svm__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# svm_params = {\n",
    "#     'svm__C': [0.0001, .001, .01],\n",
    "#     'svm__kernel': ['linear', 'rbf'],\n",
    "#     'svm__gamma': ['scale', 'auto', 0.001, 1],\n",
    "#     'svm__class_weight': [None, 'balanced']\n",
    "# }\n",
    "\n",
    "# Random Forest Hyperparameters\n",
    "rf_params = {\n",
    "    'rf__n_estimators': [100],\n",
    "    'rf__max_depth': [None, 10],\n",
    "    'rf__min_samples_split': [2],\n",
    "    'rf__min_samples_leaf': [2],\n",
    "    'rf__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# rf_params = {\n",
    "#     'rf__n_estimators': [100, 200, 500],\n",
    "#     'rf__max_depth': [None, 10, 20, 30],\n",
    "#     'rf__min_samples_split': [2, 5, 10],\n",
    "#     'rf__min_samples_leaf': [1, 2, 4],\n",
    "#     'rf__class_weight': [None, 'balanced', 'balanced_subsample']\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning Strategy\n",
    "def tune_model(pipe, params, X, y):\n",
    "    search = RandomizedSearchCV(\n",
    "        pipe,\n",
    "        params,\n",
    "        n_iter=50,\n",
    "        cv=3,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    search.fit(X, y)\n",
    "    return search.best_estimator_, search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning SVM...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuteng/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 6 is smaller than n_iter=50. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# SVM Tuning\n",
    "print(\"Tuning SVM...\")\n",
    "best_svm, svm_best_params = tune_model(svm_pipe, svm_params, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning Random Forest...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuteng/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 4 is smaller than n_iter=50. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Tuning\n",
    "print(\"\\nTuning Random Forest...\")\n",
    "best_rf, rf_best_params = tune_model(rf_pipe, rf_params, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Performance:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       akiec       1.00      1.00      1.00        65\n",
      "         bcc       1.00      1.00      1.00       103\n",
      "         bkl       1.00      1.00      1.00       220\n",
      "          df       1.00      1.00      1.00        23\n",
      "         mel       1.00      1.00      1.00       223\n",
      "          nv       1.00      1.00      1.00      1341\n",
      "        vasc       1.00      1.00      1.00        28\n",
      "\n",
      "    accuracy                           1.00      2003\n",
      "   macro avg       1.00      1.00      1.00      2003\n",
      "weighted avg       1.00      1.00      1.00      2003\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  65    0    0    0    0    0    0]\n",
      " [   0  103    0    0    0    0    0]\n",
      " [   0    0  220    0    0    0    0]\n",
      " [   0    0    0   23    0    0    0]\n",
      " [   0    0    0    0  223    0    0]\n",
      " [   0    0    0    0    0 1341    0]\n",
      " [   0    0    0    0    0    0   28]]\n",
      "\n",
      "Random Forest Performance:\n",
      "Accuracy: 0.9191\n",
      "F1 Score: 0.9117\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       akiec       1.00      0.89      0.94        65\n",
      "         bcc       0.91      0.83      0.87       103\n",
      "         bkl       0.88      0.86      0.87       220\n",
      "          df       1.00      0.13      0.23        23\n",
      "         mel       0.98      0.66      0.79       223\n",
      "          nv       0.92      1.00      0.96      1341\n",
      "        vasc       1.00      0.61      0.76        28\n",
      "\n",
      "    accuracy                           0.92      2003\n",
      "   macro avg       0.96      0.71      0.77      2003\n",
      "weighted avg       0.92      0.92      0.91      2003\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  58    4    2    0    1    0    0]\n",
      " [   0   86   12    0    0    5    0]\n",
      " [   0    1  189    0    2   28    0]\n",
      " [   0    3    8    3    0    9    0]\n",
      " [   0    0    4    0  148   71    0]\n",
      " [   0    0    1    0    0 1340    0]\n",
      " [   0    0    0    0    0   11   17]]\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Evaluate SVM\n",
    "print(\"SVM Performance:\")\n",
    "evaluate_model(best_svm, X_test, y_test)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "print(\"\\nRandom Forest Performance:\")\n",
    "evaluate_model(best_rf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Important Features:\n",
      "      feature  importance\n",
      "118     label    0.224430\n",
      "45   hist_g_6    0.018309\n",
      "78   hist_b_7    0.016769\n",
      "79   hist_b_8    0.015718\n",
      "46   hist_g_7    0.015250\n",
      "103     lbp_0    0.014948\n",
      "77   hist_b_6    0.014408\n",
      "106     lbp_3    0.014370\n",
      "80   hist_b_9    0.014327\n",
      "108     lbp_5    0.013847\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance Analysis (RF Specific)\n",
    "\n",
    "# Get feature importances\n",
    "importances = best_rf.named_steps['rf'].feature_importances_\n",
    "feature_names = X.columns\n",
    "feat_imp = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "feat_imp = feat_imp.sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(feat_imp.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "joblib.dump(best_svm, 'best_svm_model.pkl')\n",
    "joblib.dump(best_rf, 'best_rf_model.pkl')\n",
    "joblib.dump(le, 'label_encoder.pkl')\n",
    "\n",
    "# Save feature names\n",
    "with open('feature_names.txt', 'w') as f:\n",
    "    f.write('\\n'.join(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Parameter Adjustment Strategies\n",
    "\"\"\"\n",
    "For SVM:\n",
    "1. Regularization (C): \n",
    "   - Start with log scale values (0.1, 1, 10, 100)\n",
    "   - Higher C = less regularization, might overfit\n",
    "   \n",
    "2. Kernel Selection:\n",
    "   - Try linear first for baseline\n",
    "   - RBF for non-linear relationships\n",
    "   - Poly for complex patterns (but needs more data)\n",
    "   \n",
    "3. Gamma:\n",
    "   - Controls decision boundary curvature\n",
    "   - Lower values = larger influence radius\n",
    "   - Use 'scale' (1/(n_features * X.var())) as baseline\n",
    "\n",
    "For Random Forest:\n",
    "1. n_estimators:\n",
    "   - Start with 100-500 trees\n",
    "   - More trees = better performance but longer training\"\n",
    "\n",
    "2. max_depth:\n",
    "   - Control tree complexity\n",
    "   - None for full expansion (watch for overfitting)\n",
    "   \n",
    "3. class_weight:\n",
    "   - Crucial for imbalanced datasets\n",
    "   - 'balanced' adjusts weights inversely proportional to class frequencies\n",
    "   \n",
    "4. min_samples_split:\n",
    "   - Higher values prevent overfitting\n",
    "   - Start with 2 (default), try 5-10 for regularization\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
